---
layout:     post
title:      "Semi-supervised Learning"
subtitle:   "Semi-supervised Learning"
date:       2019-08-06
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Semi-supervised Learning
---  
## 1. Introduction
![](/img/lhy_ml/ssl-1.png)  ![](/img/lhy_ml/ssl-2.png)  
我们缺的不是数据，而是有标签的数据，无标签的数据，在一定的假设下，其分布可以告诉我们一些信息，这是半监督起作用的基础

## 2. ssl for Generative model
![](/img/lhy_ml/ssl-3.png)  
用有标签的数据初始化μ等参数，计算得到model，用这个model预测无标签的数据，无标签的数据可能属于class1可能属于class2，根据这个可能性计算新的μ，
更新model

## 3. Assumption

### Low-density Separation"非黑即白"
![](/img/lhy_ml/ssl-8.png)  
low-density表示分类的边界是稀疏的

#### self-training
![](/img/lhy_ml/ssl-4.png)  
self-train对regression没有作用，因为regression是预测一个实数，这个实数无法更新model

![](/img/lhy_ml/ssl-5.png)  
如果使用NN，对比generation model，soft label不起作用，因为新的model的学习目标是旧model的输出，是无法学习的，但是generation model可以，是
因为它是通过更新μ来更新model的

* Entropy-based Regularization
![](/img/lhy_ml/ssl-6.png)  
有时直接认为p=0.7就是class1(图中例子)不太可靠，可以使用Entropy-based Regularization方式，loss function要求，有标签的尽可能和标签结果相同，无标签得到的类别可能性尽可能集中

#### Semi-supervised SVM
![](/img/lhy_ml/ssl-7.png)  
对所有数据穷举所有可能的分类方法，用SVM找到每个分法的分类方式，最后比较这些分类方式，找到有标签分类正确并且margin最大的划分方式

### Smoothness
![](/img/lhy_ml/ssl-10.png)  ![](/img/lhy_ml/ssl-9.png)  
直接看会发现第二个2和3更像，但是第一个2到第二个2之间有过渡，而第二个2和3之间没有，不smooth，所以我们认为第二个2是2而不是3，这就是Smoothness Assumption

#### Cluster and label
![](/img/lhy_ml/ssl-11.png)  
所有的Data做cluster，要数据可以cluster才可以，两个橘色的点之间有其他很多点，他们俩可以过渡过去，而橘色的点与绿色的点中间点少，难以跨过去

#### Graph-based Approach
![](/img/lhy_ml/ssl-12.png)  
通过基于图的方法来分类，将数据表示为图，如果图上是相连的就是有high density path可以走到的，有时数据自然表示为图，有时需要自己构建图

![](/img/lhy_ml/ssl-13.png)  
构建图的方法举例如上(需要自己根据经验来决定怎么构建)，Radial Basis function表示两个点距离越近s值越大，距离稍远s就会变得很小，用来作为边的权重，图中橘色和绿色距离比两个橘色大一点，但是就不再相连

![](/img/lhy_ml/ssl-14.png)  
有了图之后，分类的方式基于传染，一个节点连接了有class1标签的节点，class1会传给图上相连的其他点

上面是定性的方法过程，以下是该方法的定量过程：
![](/img/lhy_ml/ssl-15.png)  
![](/img/lhy_ml/ssl-16.png)  
![](/img/lhy_ml/ssl-17.png)  
Loss funtion要求有标签数据尽可能符合标签，无标签数据尽可能smooth，衡量smooth的计算方式如上，通过减小loss，来构建图

### Better Representation
