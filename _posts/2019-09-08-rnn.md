---
layout:     post
title:      "Deep Learning"
subtitle:   "RNN"
date:       2019-09-08
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Deep Learning
---  
## RNN
![](/img/lhy_ml/rnn-1.jpg)  
我们以slot filling为例，知道一句话中哪些词汇是哪个slot

将词汇用向量表示：  
![](/img/lhy_ml/rnn-2.jpg)  ![](/img/lhy_ml/rnn-3.jpg)  

![](/img/lhy_ml/rnn-4.jpg)  
直接使用普通的网络处理这个问题会有问题，我们需要记忆前面的输入

![](/img/lhy_ml/rnn-5.jpg)  
RNN架构如上，记忆a1,a2有初始值，输入**x<sup>1</sup>**=(x<sub>1</sub>,x<sub>2</sub>,...)，
第一个绿色神经元计算x<sub>1</sub>、x<sub>2</sub>、a1、a2作为输入的结果，同理第二个绿色神经元
计算x<sub>1</sub>、x<sub>2</sub>、a1、a2作为输入的结果，然后红色的神经元计算输出值，最后将绿色神经元
的输出结果存到a1、a2中，接下来输入第二个输入x<sup>2</sup>，用新的记忆进行计算

![](/img/lhy_ml/rnn-6.jpg)  
上图是同一个网络在不同的时间点的使用，可以看出来不同顺序下同一输入的结果会不同

![](/img/lhy_ml/rnn-7.jpg)  

![](/img/lhy_ml/rnn-8.jpg)  
由于绿色层没有target，我们不知道它输出的意义，可以去记忆输出层的输出，我们知道输出层的target，也就知道放到记忆中的是什么

![](/img/lhy_ml/rnn-9.jpg)  
双向RNN

## LSTM
![](/img/lhy_ml/rnn-10.jpg)  
在记忆cell增加了三个门，记忆cell有四个输入：记忆内容、三个门的控制信号(0/1)  
long short-term：长的短期记忆网络

![](/img/lhy_ml/rnn-11.jpg)  
c'是新的记忆，遗忘门打开时是存新的记忆，关闭时遗忘记忆，f(z)取0/1，计算的例子如下：  
![](/img/lhy_ml/rnn-12.jpg)  
图中上一个记忆为0

**LSTM与RNN**  
![](/img/lhy_ml/rnn-13.jpg)  
把LSTM看做一个需要四个输入神经元，它的参数是普通神经元的4倍
![](/img/lhy_ml/rnn-14.jpg)  
上面一排多个LSTM可以看作是一层网络，每个LSTM是一个神经元，输入x向量(序列中一个值的vector表示)，乘以4个矩阵/参数，得到四个z向量作为LSTM的输入，对于一个LSTM/一个神经元，输入的是z的一个维度
![](/img/lhy_ml/rnn-15.jpg)  
一层LSTM可以同时计算，一层LSTM对应的是序列中一个值作为输入  
![](/img/lhy_ml/rnn-16.jpg)  
整个序列多个值输入过程如上，序列下一个值输入时还要和上一个值的输出和上一次的记忆一起来作为输入
![](/img/lhy_ml/rnn-18.jpg)  
LSTM也可以是多层
![](/img/lhy_ml/rnn-17.jpg)  
karas有LSTM(一般说RNN都是指LSTM)、GRU、SimpleRNN(最开始讲的RNN)的资源

## Learning
![](/img/lhy_ml/rnn-21.jpg)  
![](/img/lhy_ml/rnn-19.jpg)  
用BPTT进行参数更新  
![](/img/lhy_ml/rnn-20.jpg)  
有时RNN会出现上图中波动很大的情况，这是因为RNN的error surface要么非常平坦要么非常陡峭：  
![](/img/lhy_ml/rnn-22.jpg)  
图上显示的是w1w2两个参数对toral loss的影响，如果踩在悬崖上，梯度会非常大，会直接飞出去，用clipping解决: 规定梯度>15时都认为是15  
为什么RNN会出现error surface很崎岖的问题？
![](/img/lhy_ml/rnn-23.jpg)  
之前的梯度消失问题是因为用了sigmoid，如果是这个原因，那么ReLU就可以解决这个问题，但是在RNN中用ReLU表现不好，所以这里不是sigmoid的问题  
从memery接到下一个神经元的权重为w，最开始输入1，上面的网络最后输出y<sup>1000</sup>=w<sup>999</sup>，我们稍微改变w的值，来看w的梯度大小，可以看到w在很小的范围里可能有很大的梯度，需要小学习率，也可能有很小的梯度，需要大学习率，
这样导致error suface很崎岖，并且难以设定学习率，也就是说在时间和时间转换过程中w会反复使用，使得w如果会造成影响就会有很大的影响，有时又没有什么影响，这就是造成RNN出现上面问题的原因  
那么如何解决这个问题呢？  
![](/img/lhy_ml/rnn-24.jpg)  
LSTM可以解决这个问题，它让error surface没有很平坦的地方，解决梯度消失的问题，(但是不会去掉很崎岖的地方，不会解决梯度爆炸的问题)，这样我们可以放心将学习率设置的很小  

为什么把RNN换成LSTM?就是因为LSTM？  
因为LSTM可以解决梯度消失的问题，它们处理记忆的方法不同，RNN每个时间的记忆都会被洗掉，但是LSTM是记忆相加，如果w可以影响到记忆中的值，这个影响会一直存在，
而RNN会失去这个影响，对于LSTM的遗忘门，最开始的LSTM就是为了解决梯度消失问题，是没有遗忘门的，现在的遗忘门是新加的，有传言说在训练时会一直保持遗忘门的bias很大，让遗忘门尽量开启，
现在也有另一个版本GRU，只有两个gate，把输入门和遗忘门联动，输入门打开时遗忘门关闭，输入门关闭时遗忘门打开，也就是忘了旧的才能记新的
![](/img/lhy_ml/rnn-25.jpg)  
可以用Clokwise RNN、SCRN等方法来解决梯度消失问题  
一般的RNN用identity matrix单位矩阵 + ReLU会表现比LSTM好

## Application
![](/img/lhy_ml/rnn-26.jpg)  
![](/img/lhy_ml/rnn-27.jpg)  

![](/img/lhy_ml/rnn-28.jpg)  
![](/img/lhy_ml/rnn-29.jpg)  
![](/img/lhy_ml/rnn-30.jpg)  
![](/img/lhy_ml/rnn-31.jpg)  

![](/img/lhy_ml/rnn-32.jpg)  
![](/img/lhy_ml/rnn-33.jpg)  

![](/img/lhy_ml/rnn-34.jpg)  
syntactic parsing问题是看句子得到文法的结构树，可以用Structured learning解决，用RNN解决时可以把树状图描述成sequence，输入句子，得到树的
sequence

**Sequence-to-sequence**  
用在文字上：  
![](/img/lhy_ml/rnn-35.jpg)  
理解语义
![](/img/lhy_ml/rnn-36.jpg)  
无需label，输入输出相同方便理解语法，也可以让输出是下一句，方便理解语义，可用于聊天机器人
![](/img/lhy_ml/rnn-37.jpg)  
多层，把输出的多个句子各自embedding，再相加，后分解，用分解后的再得到句子

用在语音上：  
![](/img/lhy_ml/rnn-38.jpg)  
![](/img/lhy_ml/rnn-39.jpg)  
用于语音搜索

## Attention-based Model
Attention：用了记忆的其他网络  
![](/img/lhy_ml/rnn-40.jpg)  
![](/img/lhy_ml/rnn-41.jpg)  
可以用于以下问题中：  
**Reading Comprehension**  
![](/img/lhy_ml/rnn-42.jpg)  
可以从多个地方读，最后connect到一起  
**Visual QA**
![](/img/lhy_ml/rnn-43.jpg)  
可以回头修改答案  
**Speech QA**
![](/img/lhy_ml/rnn-44.jpg)  
![](/img/lhy_ml/rnn-45.jpg)  

