---
layout:     post
title:      "Unsupervised Learning"
subtitle:   "Deep Auto-encoder"
date:       2019-09-02
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Unsupervised Learning
---  
## 1.Deep Auto-encoder
![](/img/lhy_ml/auto-1.jpg)  
输出是否与输入很像不是最重要的，最重要的是得到的code能不能学到东西，如果中间code维度比输入大，有可能coder只是输入的扩充，
这样输入与输出会很像但是code其实并没有学到东西，如果要做coder比输入大的，应该对参数进行限制，或者使用De-noising auto-encoder  
![](/img/lhy_ml/auto-3.jpg)  
将数据加上noise后作为输入，但是最后的输出要和原始数据一样  
![](/img/lhy_ml/auto-2.jpg)  
PCA与auto-encoder相似，但是encoder与decoder互为转置，且只有一‘层’

## 2.Application
#### Text Retrieval
![](/img/lhy_ml/auto-4.jpg)  
![](/img/lhy_ml/auto-5.jpg)  
文本搜索原理是将文档与query(查询内容)同时Embedding，计算query与文档之间的距离，而是用bag-of-word来表示句子(文档、query)会失去语义信息，
所以将bag-of-word输入到auto-encoder中，得到心得Embedding表示

#### Similar Image Search
![](/img/lhy_ml/auto-6.jpg)  

#### CNN
![](/img/lhy_ml/auto-7.jpg)  
![](/img/lhy_ml/auto-8.jpg)  
pooling时记住最大的在哪个位置(左上角..)，unpooling时将最大的还原为原来位置，其他位置补零  
![](/img/lhy_ml/auto-9.jpg)  
灰色表示值为0，deconvolution相当于补零之后做convolution

#### Pre-train DNN
![](/img/lhy_ml/auto-11.jpg)  
![](/img/lhy_ml/auto-12.jpg)  
![](/img/lhy_ml/auto-13.jpg)  
![](/img/lhy_ml/auto-14.jpg)  
我们要做一个左边的目标网络来做某个任务，这个任务有大量的数据，但是只有少量的有标签，我们可以用auto-encoder做预训练，
首先做一个一层1000神经元的auto-encoder网络，训练得到w1，再做两层1000神经元的网络训练得到w2，同理得到w3，用得到的w1，w2，w3作为网络的初始参数，
在最后接上随机生成参数的输出层，做DNN的训练

#### 生成图片
![](/img/lhy_ml/auto-10.jpg)  
利用decoder将code生成图片


