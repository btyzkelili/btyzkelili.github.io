---
layout:     post
title:      "强化学习:Q-learning"
subtitle:   ""
date:       2019-05-29 12:00:00
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - 强化学习
---
## 1.Reinforcement Learning
* 强化学习与监督学习：监督学习是已经有了数据和数据对应的正确标签。而强化学习还要更进一步, 一开始它并没有数据和标签，要通过一次次在环境中的尝试, 获取这些数据和标签, 然后再学习通过哪些数据能够对应哪些标签, 通过学习到的这些规律, 竟可能地选择带来高分的行为，这也就证明了在强化学习中, 分数标签就是他的老师, 他和监督学习中的老师也差不多.
![](/img/rl/1-7.jpg)  
## 1.Q-learning
![](/img/rl/1-3.jpg)  
![](/img/rl/1-2.jpg)   
* 说明：  
1. 更新Q值时，用了S'状态下可得到**最大**奖励动作a的reward，但是在到下一轮S'状态时，并没有真的采取动作a，而是重新根据策略(eg.e-greedy)选择动作 
3. 动作选择根据当前Q-table和策略，而Q的更新是根据Q的最大奖励动作计算，说到不一定做到，因此为off-policy学习。
* on-policy和off-policy是指评估和改进的策略是否一致，如不一致则是异策略，一致则为同策略，异策略就是指不是学此时此刻的的策略, 而是学有可能和此时此刻不一样的策略. 同策略就是学此时此刻的策略。
* Q现实：s1状态下e-greedy选择动作a实际获得的奖励和a动作达到的下一个状态所能得到的最大Q值组成，因为当前能获得的reward和未来所走方向的Q值
* Q估计：s1状态下e-greedy选择动作a时的Q值，因为这个值是过去记录的，是当前根据过去状况的一个估计
## 3.Sarsa
![](/img/rl/1-4.jpg) 
![](/img/rl/1-5.jpg) 
* 说明：
1. 更新Q值时，用了S'状态下根据策略(eg.e-greedy)**所选**动作a的reward，在到下一轮S'状态时真的采取动作a
3. 动作选择根据当前Q-table和策略，而Q的更新也是根据所选动作计算，说到做到，学的是正在做的此时的策略，因此为on-policy学习。
![](/img/rl/1-6.jpg) 
* Q-learnig机器人更为勇敢，因为Q-learning机器人永远都会选择最近的一条通往成功的道路(用最大奖励更新Q表?), 不管这条路会有多危险. 而Sarsa则是相当保守, 他会选择离危险远远的（用e-greedy策略选出的动作更新Q表?）, 拿到宝藏是次要的, 保住自己的小命才是王道. 这就是使用Sarsa方法的不同之处.
## 4.实例
### RL.py  

	'''
	This part of code is the Q learning brain, which is a brain of the agent.
	All decisions are made in here.
	View more on my tutorial page: https://morvanzhou.github.io/tutorials/
	'''

	import numpy as np  
	import pandas as pd

	class RL(object):  
    
		def __init__(self, action_space, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):  
	        self.actions = action_space  # a list  
	        self.lr = learning_rate  
	        self.gamma = reward_decay  
	        self.epsilon = e_greedy  
	        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)

	    def check_state_exist(self, state):
	        if state not in self.q_table.index:
	            # append new state to q table
	            self.q_table = self.q_table.append(
	                pd.Series(
	                    [0]*len(self.actions),
	                    index=self.q_table.columns,
	                    name=state,
	                )
	            )

	    def choose_action(self, observation):
	        self.check_state_exist(observation)
	        # action selection
	        if np.random.rand() < self.epsilon:
	            # choose best action
	            state_action = self.q_table.loc[observation, :]
	            # some actions may have the same value, randomly choose on in these actions
	            action = np.random.choice(state_action[state_action == np.max(state_action)].index)
	        else:
	            # choose random action
	            action = np.random.choice(self.actions)
	        return action

	    def learn(self, *args):
	        pass


	#off-policy
	class QLearningTable(RL):
	    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
	        super(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)

	    def learn(self, s, a, r, s_):
	        self.check_state_exist(s_)
	        q_predict = self.q_table.loc[s, a]
			# q-learning与sarsa的不同处：
	        if s_ != 'terminal':
	            q_target = r + self.gamma * self.q_table.loc[s_, :].max()  # next state is not terminal
	        else:
	            q_target = r  # next state is terminal
	        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # update

	# on-policy
	class SarsaTable(RL):

	    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
	        super(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)

	    def learn(self, s, a, r, s_, a_):
	        self.check_state_exist(s_)
	        q_predict = self.q_table.loc[s, a]
			# q-learning与sarsa的不同处：
	        if s_ != 'terminal':
	            q_target = r + self.gamma * self.q_table.loc[s_, a_]  # next state is not terminal
	        else:
	            q_target = r  # next state is terminal
	        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # update

### run.py
	'''
	Sarsa is a online updating method for Reinforcement learning.
	Unlike Q learning which is a offline updating method, Sarsa is updating while in the current trajectory.
	You will see the sarsa is more coward when punishment is close because it cares about all behaviours,
	while q learning is more brave because it only cares about maximum behaviour.
	'''

	from mazenv import Maze
	from sarsa import SarsaTable

	def update():
	    for episode in range(100):
	        #initial observation
	        observation = env.reset()
	
	        #RL choose action based on observation
	        action = RL.choose_action(str(observation))
	
	        while True:
	            #fresh env
	            env.render()
	
	            #RL take action and get next observation and reward
	            observation_, reward, done = env.step(action)
	
	            #RL choose action based on next observation
	            action_ = RL.choose_action(str(observation_))
	
	            #RL learn from this transition (s, a, r, s, a) ==> Sarsa
	            RL.learn(str(observation), action, reward, str(observation_), action_)
	
	            #swap observation and action
	            observation = observation_
	            action = action_
	
	            #break while loop when end of this episode
	            if done:
	                break
	
	    #end of game
	    print('game over')
	    env.destroy()
	
	if __name__ == "__main__":
	    env = Maze()
	    RL = SarsaTable(actions=list(range(env.n_actions)))
	
	    env.after(100, update)
	    env.mainloop()

### env.py

	Reinforcement learning maze example.
	Red rectangle:          explorer.
	Black rectangles:       hells       [reward = -1].
	Yellow bin circle:      paradise    [reward = +1].
	All other states:       ground      [reward = 0].
	This script is the environment part of this example.
	The RL is in RL_brain.py.
	View more on my tutorial page: https://morvanzhou.github.io/tutorials/
	
	import numpy as np
	import time
	import sys
	if sys.version_info.major == 2:
	    import Tkinter as tk
	else:
	    import tkinter as tk
	
	
	UNIT = 40   #pixels
	MAZE_H = 4  #grid height
	MAZE_W = 4  #grid width
	
	
	class Maze(tk.Tk, object):
	    def __init__(self):
	        super(Maze, self).__init__()
	        self.action_space = ['u', 'd', 'l', 'r']
	        self.n_actions = len(self.action_space)
	        self.title('maze')
	        self.geometry('{0}x{1}'.format(MAZE_H * UNIT, MAZE_H * UNIT))
	        self._build_maze()
	
	    def _build_maze(self):
	        self.canvas = tk.Canvas(self, bg='white',
	                           height=MAZE_H * UNIT,
	                           width=MAZE_W * UNIT)
	
	        # create grids
	        for c in range(0, MAZE_W * UNIT, UNIT):
	            x0, y0, x1, y1 = c, 0, c, MAZE_H * UNIT
	            self.canvas.create_line(x0, y0, x1, y1)
	        for r in range(0, MAZE_H * UNIT, UNIT):
	            x0, y0, x1, y1 = 0, r, MAZE_W * UNIT, r
	            self.canvas.create_line(x0, y0, x1, y1)
	
	        # create origin
	        origin = np.array([20, 20])
	
	        # hell
	        hell1_center = origin + np.array([UNIT * 2, UNIT])
	        self.hell1 = self.canvas.create_rectangle(
	            hell1_center[0] - 15, hell1_center[1] - 15,
	            hell1_center[0] + 15, hell1_center[1] + 15,
	            fill='black')
	        # hell
	        hell2_center = origin + np.array([UNIT, UNIT * 2])
	        self.hell2 = self.canvas.create_rectangle(
	            hell2_center[0] - 15, hell2_center[1] - 15,
	            hell2_center[0] + 15, hell2_center[1] + 15,
	            fill='black')
	
	        # create oval
	        oval_center = origin + UNIT * 2
	        self.oval = self.canvas.create_oval(
	            oval_center[0] - 15, oval_center[1] - 15,
	            oval_center[0] + 15, oval_center[1] + 15,
	            fill='yellow')
	
	        # create red rect
	        self.rect = self.canvas.create_rectangle(
	            origin[0] - 15, origin[1] - 15,
	            origin[0] + 15, origin[1] + 15,
	            fill='red')
	
	        # pack all
	        self.canvas.pack()
	
	    def reset(self):
	        self.update()
	        time.sleep(0.5)
	        self.canvas.delete(self.rect)
	        origin = np.array([20, 20])
	        self.rect = self.canvas.create_rectangle(
	            origin[0] - 15, origin[1] - 15,
	            origin[0] + 15, origin[1] + 15,
	            fill='red')
	        # return observation
	        return self.canvas.coords(self.rect)
	
	    def step(self, action):
	        s = self.canvas.coords(self.rect)
	        base_action = np.array([0, 0])
	        if action == 0:   # up
	            if s[1] > UNIT:
	                base_action[1] -= UNIT
	        elif action == 1:   # down
	            if s[1] < (MAZE_H - 1) * UNIT:
	                base_action[1] += UNIT
	        elif action == 2:   # right
	            if s[0] < (MAZE_W - 1) * UNIT:
	                base_action[0] += UNIT
	        elif action == 3:   # left
	            if s[0] > UNIT:
	                base_action[0] -= UNIT
	
	        self.canvas.move(self.rect, base_action[0], base_action[1])  # move agent
	
	        s_ = self.canvas.coords(self.rect)  # next state
	
	        # reward function
	        if s_ == self.canvas.coords(self.oval):
	            reward = 1
	            done = True
	            s_ = 'terminal'
	        elif s_ in [self.canvas.coords(self.hell1), self.canvas.coords(self.hell2)]:
	            reward = -1
	            done = True
	            s_ = 'terminal'
	        else:
	            reward = 0
	            done = False
	
	        return s_, reward, done
	
	    def render(self):
	        time.sleep(0.1)
	        self.update()
