---
layout:     post
title:      "Reinforcement Learning"
subtitle:   "Reinforcement Learning"
date:       2019-09-10
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Reinforcement Learning
---  
## Reinforcement Learning
![](/img/lhy_ml/drl-1.jpg)  
state就是observation
![](/img/lhy_ml/drl-3.jpg)  
少数的action有reward  
alpha go训练两个网络互相下棋，下棋的胜利失败有规则

**Supervised vs RL**  
![](/img/lhy_ml/drl-4.jpg)  
有监督学习：看到棋盘，告诉下一步怎么走

**chat-bot**  
![](/img/lhy_ml/drl-5.jpg)  
![](/img/lhy_ml/drl-6.jpg)  
train两个网络互相讲话，由于对话的好坏没有像下棋一样能知道胜利还是失败，现有paper中用的方法是定义对话讲的好坏的规则，
也可以用GAN来学习reward function
![](/img/lhy_ml/drl-7.jpg)  

**interactive retrieval**
![](/img/lhy_ml/drl-10.jpg)  
检索机器反问人问题来得到具体的使用者真正想检索的问题

![](/img/lhy_ml/drl-8.jpg)  
reward delay：机器需要有远见，最终的reward是由前面步骤铺垫得到的  
机器所采取的的动作会影响未来的观察，所以机器要学会探索没有做过的行为

![](/img/lhy_ml/drl-9.jpg)  
model-based方法：预测未来发生的事

#### Policy-based(learning Actor)
![](/img/lhy_ml/drl-11.jpg)  
![](/img/lhy_ml/drl-2.jpg)  
actor就是function f(observation)=action，通过reward找function，actor也叫做policy

![](/img/lhy_ml/drl-12.jpg)  
![](/img/lhy_ml/drl-14.jpg)  
![](/img/lhy_ml/drl-15.jpg)  
判断function的好坏，由于游戏本身有随机性而且action选择有概率，使得同一个状态下选择的action有可能不同，
所以我们要最大化total reward的期望值

![](/img/lhy_ml/drl-13.jpg)  
![](/img/lhy_ml/drl-16.jpg)  
![](/img/lhy_ml/drl-17.jpg)  
![](/img/lhy_ml/drl-18.jpg)  
可以看Policy gradient那一节

![](/img/lhy_ml/drl-19.jpg)  
看到的observation有多好
