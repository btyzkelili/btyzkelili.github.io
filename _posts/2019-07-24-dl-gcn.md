---
layout:     post
title:      "Deep Learning"
subtitle:   "GCN"
date:       2019-07-24
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Deep Learning
---  
# 图卷积网络(GCN)

## 1. CNN与GCN
#### 1.1 处理数据结构不同
欧几里得结构：CNN应用在具有网格结构的数据(Euclidean Domain Data)上，比如图像(图片结构上的平移不变性：一个小窗口无论移动到图片的哪一个位置，其内部的结构都是一模一样的，因此CNN可以实现参数共享，这就是CNN的精髓所在)，音频，句子等。

拓扑结构(图结构)：现实生活中有很多非网格结构的数据，结点连接方式不同(图的结构一般来说是十分不规则的，可以认为是无限维的一种数据，所以它没有平移不变性。每一个节点的周围结构可能都是独一无二的)，例如社交网络、蛋白质结构、知识图谱等。
对于这样的数据，产生了很多解决办法，例如GNN、DeepWalk、node2vec、GCN等。

#### 1.2 特征提取
CNN使用卷积提取特征：CNN用共享参数的过滤器通过计算中心像素点以及相邻像素点的加权和构成feature map，加权系数=卷积核的权重系数，卷积的目的：空间特征的提取，确定卷积核的系数：随机化初值，训练中根据误差函数loss，通过反向传播+梯度下降进行迭代优化。  

那么图结构如何进行特征提取呢？
1. vertex domain(空域方法)
直接用相应顶点连接的neighbors来提取特征
http://proceedings.mlr.press/v48/niepert16.pdf 
![](/img/dl_gcn/15.png)  

2. spectral domain频域方法(谱方法)-->GCN
GSP（graph signal processing）图形信号处理的方法，将图当做信号，在图上进行信号处理的变换，如傅立叶变换或者拉普拉斯变换，进而进行图的卷积，从而提取图的特征。借助于图的拉普拉斯矩阵的特征值和特征向量来研究Graph的性质。

**卷积可以进行特征提取，图域卷积即对图提取到的特征，所以我们的目的就是计算图卷积，推导出图卷积公式后可以使用神经网络求解，即GCN**

## 2. GCN理论支持
#### 2.1 图的拉普拉斯矩阵
![](/img/dl_gcn/16.png)  
拉普拉斯矩阵 L = D-A  
D：度矩阵，对角矩阵  
A：邻接矩阵
图的拉普拉斯矩阵的物理意义：图的性质体现在D和A上，而拉普拉斯矩阵与图的性质满足L=D-A，即图的性质可以通过拉普拉斯矩阵体现出来，也就是说，对图的分析可以变为对拉普拉斯矩阵的分析，**图的性质<--->图的拉普拉斯矩阵的性质**

#### 2.2 拉普拉斯矩阵的特征分解/谱分解/对角化
矩阵可以特征分解的充要条件为n阶方阵存在n个线性无关的特征向量
拉普拉斯矩阵是半正定矩阵(半正定矩阵是对称矩阵)，有以下三个性质：
1. 对称矩阵一定有n个线性无关的特征向量
2. 本正定矩阵的特征值一定非负
3. 对称矩阵的特征向量相互正交，即所有特征向量构成的矩阵为正交矩阵
所以，拉普拉斯矩阵一定可以谱分解，且分解后有特殊形式，对拉普拉斯矩阵谱分解为：
![](/img/dl_gcn/17.png)  ![](/img/dl_gcn/18.png)  
其中，U是列向量为单位特征向量的矩阵，也就是说**u**<sub>i</sub>是列向量，中间部分是n个特征值构成的对角矩阵  
由于U是正交矩阵，即UU<sup>T</sup>=E，所以特征分解又可写成：
![](/img/dl_gcn/19.png)  
最右边是特征矩阵U的逆，只是拉普拉斯的性质才可写成特征矩阵的转置

#### 2.3 傅里叶变换
为什么要引入傅立叶变换？因为傅立叶变换具有一定的性质，就是原域进行卷积，相当于频域相乘，即一个域相乘，相当于另一个域卷积；一个域卷积，相当于另一个域相乘。算图域卷积(图与卷积核做卷积)相当于傅立叶域相乘，那先对图和卷积核做傅立叶变换后相乘，再傅立叶反变换回来，就得到了图域卷积。

#### 2.4 傅里叶变换与拉普拉斯矩阵的关系
传统傅立叶变换的基，就是拉普拉斯矩阵的一组特征向量，我们要求图的傅立叶变换，就要知道图的傅里叶变换的的基，就是求图的拉普拉斯矩阵的特征向量(4.2中求解得到的U)。
所以，图的傅里叶变为
![](/img/dl_gcn/20.png)  
图的第i个点上的signal为f(i)  
图x表示为x=(f(1)...f(n))在R<sup>n</sup>空间内  
互为对偶向量u<sub>l</sub><sup>*</sup>是u<sub>l</sub>的对偶向量（线性代数的知识）  
其中u<sub>l</sub><sup>*</sup>是矩阵U<sup>T</sup>的第l行，u<sub>l</sub>是矩阵U的第l行  

将图的傅里叶变换写为矩阵形式：
![](/img/dl_gcn/21.png)  
反傅里叶变换形式：
![](/img/dl_gcn/22.png)  

#### 2.4 图卷积
根据以上过程，可以得到图卷积为：
![](/img/dl_gcn/23.png)  
g是filter函数，也就是卷积核  
图x表示为x=(f(1)...f(n))∈R<sup>n</sup>，即图在每一点上面的信号  
U是傅立叶变换的基，也是拉普拉斯矩阵的特征向量  

其中理论基础为：算图域卷积相当于傅立叶域相乘，那先对图和卷积核做傅立叶变换后相乘，再傅立叶反变换回来，就得到了图域卷积。 
傅立叶变换和反傅立叶变换可以写成矩阵相乘的形式，那么对应的图卷积就写成了矩阵乘形式。

## 3. 图卷积网络(GCN)

Semi-supervised classification with graph convolutional networks, ICLR2017  ， 
https://arxiv.org/pdf/1609.02907.pdf

图x与卷积核进行卷积可以写成：
![](/img/dl_gcn/24.png)  
把U<sup>T</sup>g看做是g<sub>θ</sub>(Λ)的一个laplacian特征值的函数，参数为θ
第二个等号的等价在于特征向量与矩阵相乘等于特征向量与特征值多项式g<sub>θ</sub>(Λ)相乘。
为了省去大量求特征值特征向量这些运算，我们只对filter函数做近似，这步就是一个为了减少算法复杂度的近似：
![](/img/dl_gcn/25.png)  
也是：
![](/img/dl_gcn/26.png)  
其中T<sub>k</sub>是Chebyshev多项式，K=1就是用切比雪夫多项式进行一阶近似。
可以把g<sub>θ</sub>(Λ)简单看成是Λ的多项式，因为UΛ<sup>k</sup>U<sup>T</sup>=(UΛU<sup>T</sup>)<sup>k</sup>=L<sup>k</sup>

#### 3.1 卷积层公式
设K=1，则卷积层化简为
![](/img/dl_gcn/27.png)  ![](/img/dl_gcn/28.png)  
加上激活层，网络层与层之间传播的方式是：
![](/img/dl_gcn/1.png)  
设图有N个结点，每个节点有各自的特征，组成N * D维的特征矩阵X，各结点之间形成N * N维的邻接矩阵A，
A˜：A + I<sub>N</sub>(单位矩阵)  
D˜：A˜的度矩阵(degree matrix)，![](/img/dl_gcn/2.png)    
H<sup>l</sup>：第l层的输入特征，对于输入层，H=X  
σ：非线性激活函数  
其中，D˜<sup>-1/2</sup>A˜D˜<sup>-1/2</sup>可以预先计算出来，因为D˜由A˜得到，A˜由A和I得到，A是输入。  

#### 3.2 图卷积网络公式
![](/img/dl_gcn/30.png)![](/img/dl_gcn/3.png)![](/img/dl_gcn/4.png)![](/img/dl_gcn/29.png)  
隐层的feature maps的数量为H，输入层数量为C，输出层为F    
权重W<sup>(0)</sup>∈R<sup>C*H</sup>为输入层到隐层的权值矩阵  
同理，权重W<sup>(1)</sup>∈R<sup>H*F</sup>为隐层到输出层的权值矩阵  
与BP相似，比BP多了一个稀疏的adj matrix A  
X和A作为网络的输入，整个训练过程中，**A不变**，输入结点的特征经过GCN之后从X变为Z，对所有带标签的节点计算cross entropy损失函数来调整网络参数，使得输出的Z更好的描述结点

## 4. GCN理论high level
假设图中没有自环，即A<sub>ii</sub>=0：  
GCN本质是通过网络(graph)中的关联信息(edge)来得到节点的特征(node represent)  

假如说，我们要根据X<sub>i</sub>的朋友圈来预测X<sub>i</sub>的工资：
![](/img/dl_gcn/6.png)  

1. 首先考虑，所有朋友的工资的平均值就是自己的工资，所以(平均数值需要对加和的系数归一化即可，后面完善)：
![](/img/dl_gcn/5.png)  
如果是无权图，A<sub>ij</sub>是0/1，如果j不在neighbor(i)中，A<sub>ij</sub>=0,所以，上式可写为：
![](/img/dl_gcn/7.png)  
矩阵形式：
![](/img/dl_gcn/8.png)  

2. 平均方法有一个问题：没有考虑到我们与不同朋友的亲密度是不同的，比如，我们都认识马化腾，但是张志东与马化腾的亲密度要比我们和马化腾的亲密度高得多。因此，可以预测张志东的工资比我们更接近马化腾。
严谨来说，上式忽略了边的权重。我们只需要把无权图(A<sub>ij</sub>只能为0或1)变为有权图(A<sub>ij</sub>可以为任何数)。有很多工作都是在研究如何更巧妙的构建有权图(比如用节点间的相似度等),我们仍可以用上式表达加权平均:
![](/img/dl_gcn/8.png)  

3. 加权平均法也一个小问题：它没有考虑到『你可能很会吹牛』。比如，虽然我们能力和朋友们差不多，但是我们很会吹牛逼所以特别受到领导赏识（误）被提拔的很快。这时，可以预测我们的工资会比我们的朋友要高。
严谨来说，上式忽略了节点自身的特征。因此一般我们会通过添加一个自环把节点自身特征加回来：
![](/img/dl_gcn/9.png)  
通过A + I操作，可以将结点自身的特征加回来

4. 从另一个角度看，有一些任务对工资的绝对值不感兴趣。比如，『检测凤凰男防止相亲采坑』的任务。凤凰男可能会因为某些机遇工资飙升，但是他的朋友圈工资水平仍然不高，从而产生较大的贫富差距。而对于富二代而言，他们的朋友圈通常也是富二代，贫富差距较小。
严谨来说，有些任务可能更关注相邻节点间的“差分”。因此我们一般会用拉普拉斯矩阵L=D-A来做，其中D是图的度矩阵，节点的“差分”可以表示为：
![](/img/dl_gcn/10.png)  
从化简后的公式可以看到，通过拉普拉斯矩阵的相乘，我们求得是以边为权、节点特征的差分。

5. 归一化问题，无论是A+I还是D-A，其实存在一个重大的问题，我们求的是和而不是平均。我们可能会面临『大佬可能没有多少朋友』的问题。在聚合后，即使大佬的朋友圈都是高收入群体，也没办法超过一个拥有低端朋友圈的交际花。
![](/img/dl_gcn/11.png)  
严谨来说，这会导致离群较远或者度较小的节点在聚合后特征较小，离群较近或者度较大的节点在聚合后特征较大。因此，我们需要通过归一化消除这个问题。这里，我们令A˜=A+I或者D-A，其中D˜为A˜的度矩阵：
![](/img/dl_gcn/12.png)  
从化简后的公式可以看到，通过D˜<sup>-1</sup>操作，已经将求和变成了加权平均啦，权值之和归一化为1

6. 对称归一化，解决了平均的问题，我们能不能更进一步呢？现在朋友很少的大佬满意了。然而和朋友很少的大佬交朋友的都是什么人呢？除了高收入的程序员，还会有一些绿茶婊、交际花。表面上看，这些『交际花似乎也是大佬为数不多的朋友之一』，然而她的工资和大佬们天差地别。之所以她与大佬是朋友，是因为她跟每个人几乎都是朋友，想要扩展自己的人脉圈。因此，我们要剔除这一部分交际花对大佬工资预测的影响。
![](/img/dl_gcn/13.png)  
严谨来说，我们除了应该考虑聚合节点i的度D˜<sub>ii</sub>，还应该考虑被聚合节点j的度D˜<sub>jj</sub>。如何同时考虑两者呢，我们可以使用几何平均数：$\sqrt D˜<sub>ii</sub>D˜<sub>jj</sub>$
![](/img/dl_gcn/14.png)  
从化简后的公式可以看到，通过D˜<sup>-0.5</sup>A˜D˜<sup>-0.5</sup>操作，已经在加权时通将D˜<sup>-01</sup>拆分成了D˜<sub>ii</sub>和D˜<sub>jj</sub>的集合平均，从而剔除了被聚合节点j的度的影响。

综上，到这里 GCN 的公式已经进化成完全体了。如何想到D˜<sup>-0.5</sup>A˜D˜<sup>-0.5</sup>这种方法的呢？不难发现，这个公式就是拉普拉斯变换或者说谱聚类加了参数：
1. 首先计算出标准化后的拉普拉斯矩阵D˜<sup>-0.5</sup>(D-A)D˜<sup>-0.5</sup>
2. 将拉普拉斯矩阵进行SVD特征分解，得到U
3. 选取较小的k个U到其他聚类算法，诸如KMeans

## 5. GCN.py(Keras)
#### setup.py
```python3
from setuptools import setup
from setuptools import find_packages
 
setup(name='kegra',           # 生成的包名称
      version='0.0.1',        # 版本号
      description='Deep Learning on Graphs with Keras',           # 包的简要描述
      author='Thomas Kipf',         # 包的作者
      author_email='thomas.kipf@gmail.com',           # 包作者的邮箱地址
      url='https://tkipf.github.io',            # 程序的官网地址
      download_url='...',           # 程序的下载地址
      license='MIT',          # 程序的授权信息
      install_requires=['keras'],         # 需要安装的依赖包
      extras_require={        # 额外用于模型存储的依赖包
          'model_saving': ['json', 'h5py'],
      },
      package_data={'kegra': ['README.md']},
      # fine_packages()函数默认在和setup.py同一目录下搜索各个含有__init__.py的包
      packages=find_packages())
```
#### utils.py
```python3
# 如果某个版本中出现了某个新的功能特性，而且这个特性和当前版本中使用的不兼容，
# 也就是说它在当前版本中不是语言标准，那么我们如果想要使用的话就要从__future__模块导入

from __future__ import print_function       # print()函数
import scipy.sparse as sp       # python中稀疏矩阵相关库
import numpy as np      # python中操作数组的函数
from scipy.sparse.linalg.eigen.arpack import eigsh, ArpackNoConvergence     # 稀疏矩阵中查找特征值/特征向量的函数

# 将标签转换为one-hot编码形式
def encode_onehot(labels):
    # set()函数创建一个不重复元素集合
    classes = set(labels)
    
    # np.identity()函数创建方针，返回主对角线元素为1，其余元素为0的数组
    # enumerate()函数用于将一个可遍历的数据对象（如列表、元组或字符串）组合为一个索引序列
    # 同时列出数据和数据下标，一般用在for循环中
    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}

    # map()函数根据提供的函数对指定序列做映射
    # map(function, iterable)
    # 第一个参数function以参数序列中的每一个元素调用function函数，返回包含每次function函数返回值的新列表
    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)
    return labels_onehot
    
# 加载数据
def load_data(path="data/cora/", dataset="cora"):
    """Load citation network dataset (cora only for now)"""
    # str.format()函数用于格式化字符串
    print('Loading {} dataset...'.format(dataset))

    # np.genfromtxt()函数用于从.csv文件或.tsv文件中生成数组
    # np.genfromtxt(fname, dtype, delimiter, usecols, skip_header)
    # frame：文件名
    # dtype：数据类型
    # delimiter：分隔符
    # usecols：选择读哪几列，通常将属性集读为一个数组，将标签读为一个数组
    # skip_header：是否跳过表头
    idx_features_labels = np.genfromtxt("{}{}.content".format(path, dataset), dtype=np.dtype(str))

    # 提取样本的特征，并将其转换为csr矩阵（压缩稀疏行矩阵），用行索引、列索引和值表示矩阵
    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)

    # 提取样本的标签，并将其转换为one-hot编码形式
    labels = encode_onehot(idx_features_labels[:, -1])

    # build graph
    # 样本的id数组
    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)

    # 有样本id到样本索引的映射字典
    idx_map = {j: i for i, j in enumerate(idx)}

    # 样本之间的引用关系数组
    edges_unordered = np.genfromtxt("{}{}.cites".format(path, dataset), dtype=np.int32)

    # 将样本之间的引用关系用样本索引之间的关系表示
    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),
                     dtype=np.int32).reshape(edges_unordered.shape)

    # 构建图的邻接矩阵，用坐标形式的稀疏矩阵表示，非对称邻接矩阵
    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),
                        shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)

    # build symmetric adjacency matrix
    # 将非对称邻接矩阵转变为对称邻接矩阵
    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)

    # 打印消息：数据集有多少个节点、多少条边、每个样本有多少维特征
    print('Dataset has {} nodes, {} edges, {} features.'.format(adj.shape[0], edges.shape[0], features.shape[1]))

    # 返回特征的密集矩阵表示、邻接矩阵和标签的one-hot编码
    return features.todense(), adj, labels

# 对邻接矩阵进行归一化处理
def normalize_adj(adj, symmetric=True):
    # 如果邻接矩阵为对称矩阵，得到对称归一化邻接矩阵
    # D^(-1/2) * A * D^(-1/2)
    if symmetric:
        # A.sum(axis=1)：计算矩阵的每一行元素之和，得到节点的度矩阵D
        # np.power(x, n)：数组元素求n次方，得到D^(-1/2)
        # sp.diags()函数根据给定的对象创建对角矩阵，对角线上的元素为给定对象中的元素
        d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)

        # tocsr()函数将矩阵转化为压缩稀疏行矩阵
        a_norm = adj.dot(d).transpose().dot(d).tocsr()

    # 如果邻接矩阵不是对称矩阵，得到随机游走正则化拉普拉斯算子
    # D^(-1) * A
    else:
        d = sp.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0)
        a_norm = d.dot(adj).tocsr()
    return a_norm

# 在邻接矩阵中加入自连接
def preprocess_adj(adj, symmetric=True):
    adj = adj + sp.eye(adj.shape[0])
    # 对加入自连接的邻接矩阵进行对称归一化处理
    adj = normalize_adj(adj, symmetric)
    return adj

# 构造样本掩码
def sample_mask(idx, l):
    """
    :param idx: 有标签样本的索引列表
    :param l: 所有样本数量
    :return: 布尔类型数组，其中有标签样本所对应的位置为True，无标签样本所对应的位置为False
    """
    # np.zeros()函数创建一个给定形状和类型的用0填充的数组
    mask = np.zeros(l)
    mask[idx] = 1
    return np.array(mask, dtype=np.bool)

# 数据集划分
def get_splits(y):
    # 训练集索引列表
    idx_train = range(140)
    # 验证集索引列表
    idx_val = range(200, 500)
    # 测试集索引列表
    idx_test = range(500, 1500)
    # 训练集样本标签
    y_train = np.zeros(y.shape, dtype=np.int32)
    # 验证集样本标签
    y_val = np.zeros(y.shape, dtype=np.int32)
    # 测试集样本标签
    y_test = np.zeros(y.shape, dtype=np.int32)
    y_train[idx_train] = y[idx_train]
    y_val[idx_val] = y[idx_val]
    y_test[idx_test] = y[idx_test]
    # 训练数据的样本掩码
    train_mask = sample_mask(idx_train, y.shape[0])
    return y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask
    
# 定义分类交叉熵
def categorical_crossentropy(preds, labels):
    """
    :param preds:模型对样本的输出数组
    :param labels:样本的one-hot标签数组
    :return:样本的平均交叉熵损失
    """
    # np.extract(condition, x)函数，根据某个条件从数组中抽取元素
    # np.mean()函数默认求数组中所有元素均值
    return np.mean(-np.log(np.extract(labels, preds)))

# 定义准确率函数
def accuracy(preds, labels):
    # np.argmax(x)函数取出x中元素最大值所对应的索引
    # np.equal(x1, x2)函数用于在元素级比较两个数组是否相等
    return np.mean(np.equal(np.argmax(labels, 1), np.argmax(preds, 1)))

# 评估样本划分的损失函数和准确率
def evaluate_preds(preds, labels, indices):
    """
    :param preds:对于样本的预测值
    :param labels:样本的标签one-hot向量
    :param indices:样本的索引集合
    :return:交叉熵损失函数列表、准确率列表
    """
    split_loss = list()
    split_acc = list()
    
    for y_split, idx_split in zip(labels, indices):
        # 计算每一个样本划分的交叉熵损失函数
        split_loss.append(categorical_crossentropy(preds[idx_split], y_split[idx_split]))
        # 计算每一个样本划分的准确率
        split_acc.append(accuracy(preds[idx_split], y_split[idx_split]))
    return split_loss, split_acc

# 对拉普拉斯矩阵进行归一化处理
def normalized_laplacian(adj, symmetric=True):
    # 对称归一化的邻接矩阵，D ^ (-1/2) * A * D ^ (-1/2)
    adj_normalized = normalize_adj(adj, symmetric)

    # 得到对称规范化的图拉普拉斯矩阵，L = I - D ^ (-1/2) * A * D ^ (-1/2)
    laplacian = sp.eye(adj.shape[0]) - adj_normalized
    
    return laplacian

# 重新调整对称归一化的图拉普拉斯矩阵，得到其简化版本
def rescale_laplacian(laplacian):
    try:
        print('Calculating largest eigenvalue of normalized graph Laplacian...')
        # 计算对称归一化图拉普拉斯矩阵的最大特征值
        largest_eigval = eigsh(laplacian, 1, which='LM', return_eigenvectors=False)[0]
    # 如果计算过程不收敛
    except ArpackNoConvergence:
        print('Eigenvalue calculation did not converge! Using largest_eigval=2 instead.')
        largest_eigval = 2

    # 调整后的对称归一化图拉普拉斯矩阵，L~ = 2 / Lambda * L - I
    scaled_laplacian = (2. / largest_eigval) * laplacian - sp.eye(laplacian.shape[0])
    return scaled_laplacian

# 计算直到k阶的切比雪夫多项式
def chebyshev_polynomial(X, k):
    # 返回一个稀疏矩阵列表
    """Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices."""
    print("Calculating Chebyshev polynomials up to order {}...".format(k))

    T_k = list()
    T_k.append(sp.eye(X.shape[0]).tocsr())      # T0(X) = I
    T_k.append(X)       # T1(X) = L~

    # 定义切比雪夫递归公式
    def chebyshev_recurrence(T_k_minus_one, T_k_minus_two, X):
        """
        :param T_k_minus_one: T(k-1)(L~)
        :param T_k_minus_two: T(k-2)(L~)
        :param X: L~
        :return: Tk(L~)
        """
        # 将输入转化为csr矩阵（压缩稀疏行矩阵）
        X_ = sp.csr_matrix(X, copy=True)

        # 递归公式：Tk(L~) = 2L~ * T(k-1)(L~) - T(k-2)(L~)
        return 2 * X_.dot(T_k_minus_one) - T_k_minus_two

    for i in range(2, k+1):
        T_k.append(chebyshev_recurrence(T_k[-1], T_k[-2], X))

    # 返回切比雪夫多项式列表
    return T_k

# 将稀疏矩阵转化为元组表示
def sparse_to_tuple(sparse_mx):
    if not sp.isspmatrix_coo(sparse_mx):
        # 将稀疏矩阵转化为coo矩阵形式
        # coo矩阵采用三个数组分别存储行、列和非零元素值的信息
        sparse_mx = sparse_mx.tocoo()

    # np.vstack()函数沿着数组的某条轴堆叠数组
    # 获取非零元素的位置索引
    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()

    # 获取矩阵的非零元素
    values = sparse_mx.data
    
    # 获取矩阵的形状
    shape = sparse_mx.shape
    return coords, values, shape

```
#### graph.py
```python3

# 如果某个版本中出现了某个新的功能特性，而且这个特性和当前版本中使用的不兼容，
# 也就是说它在当前版本中不是语言标准，那么我们如果想要使用的话就要从__future__模块导入
from __future__ import print_function       # print()函数

from keras import activations, initializers, constraints
from keras import regularizers
from keras.engine import Layer
import keras.backend as K

# 定义基本的图卷积类

# Keras自定义层要实现build方法、call方法和compute_output_shape(input_shape)方法
class GraphConvolution(Layer):
    """Basic graph convolution layer as in https://arxiv.org/abs/1609.02907"""
    # 构造函数
    def __init__(self, units, support=1,
                 activation=None,
                 use_bias=True,
                 kernel_initializer='glorot_uniform',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 activity_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 **kwargs):

        if 'input_shape' not in kwargs and 'input_dim' in kwargs:
            # pop()函数用于删除列表中某元素，并返回该元素的值
            kwargs['input_shape'] = (kwargs.pop('input_dim'),)
            
        super(GraphConvolution, self).__init__(**kwargs)
        self.units = units
        self.activation = activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = initializers.get(kernel_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        # 施加在权重上的正则项
        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        # 施加在偏置向量上的正则项
        self.bias_regularizer = regularizers.get(bias_regularizer)
        # 施加在输出上的正则项
        self.activity_regularizer = regularizers.get(activity_regularizer)
        # 对主权重矩阵进行约束
        self.kernel_constraint = constraints.get(kernel_constraint)
        # 对偏置向量进行约束
        self.bias_constraint = constraints.get(bias_constraint)
        self.supports_masking = True
        self.support = support
        assert support >= 1

    # 计算输出的形状
    # 如果自定义层更改了输入张量的形状，则应该在这里定义形状变化的逻辑
    # 让Keras能够自动推断各层的形状
    def compute_output_shape(self, input_shapes):
        # 特征矩阵形状
        features_shape = input_shapes[0]
        # 输出形状为(批大小, 输出维度)
        output_shape = (features_shape[0], self.units)
        return output_shape  # (batch_size, output_dim)
        
    # 定义层中的参数
    def build(self, input_shapes):
        # 特征矩阵形状
        features_shape = input_shapes[0]
        assert len(features_shape) == 2
        # 特征维度
        input_dim = features_shape[1]
        self.kernel = self.add_weight(shape=(input_dim * self.support,
                                             self.units),
                                      initializer=self.kernel_initializer,
                                      name='kernel',
                                      regularizer=self.kernel_regularizer,
                                   constraint=self.kernel_constraint)
        # 如果存在偏置
        if self.use_bias:
            self.bias = self.add_weight(shape=(self.units,),
                                        initializer=self.bias_initializer,
                                        name='bias',
                                        regularizer=self.bias_regularizer,
                                        constraint=self.bias_constraint)
        else:
            self.bias = None
        # 必须设定self.bulit = True
        self.built = True

    # 编写层的功能逻辑
    def call(self, inputs, mask=None):
        features = inputs[0]        # 特征
        basis = inputs[1:]      # 对称归一化的邻接矩阵

        # 多个图的情况
        supports = list()
        for i in range(self.support):
            # A * X
            supports.append(K.dot(basis[i], features))
        # 将多个图的结果按行拼接
        supports = K.concatenate(supports, axis=1)
        # A * X * W
        output = K.dot(supports, self.kernel)

        if self.bias:
            # A * X * W + b
            output += self.bias
        return self.activation(output)

    # 定义当前层的配置信息
    def get_config(self):
        config = {'units': self.units,
                  'support': self.support,
                  'activation': activations.serialize(self.activation),
                  'use_bias': self.use_bias,
                  'kernel_initializer': initializers.serialize(
                      self.kernel_initializer),
                  'bias_initializer': initializers.serialize(
                      self.bias_initializer),
                  'kernel_regularizer': regularizers.serialize(
                      self.kernel_regularizer),
                  'bias_regularizer': regularizers.serialize(
                      self.bias_regularizer),
                  'activity_regularizer': regularizers.serialize(
                      self.activity_regularizer),
                  'kernel_constraint': constraints.serialize(
                      self.kernel_constraint),
                  'bias_constraint': constraints.serialize(self.bias_constraint)
        }

        base_config = super(GraphConvolution, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
```
#### train.py
```python3
from __future__ import print_function
from keras.layers import Input, Dropout
from keras.models import Model
from keras.optimizers import Adam
from keras.regularizers import l2

from kegra.layers.graph import GraphConvolution
from kegra.utils import *\

import time

# 超参数
# Define parameters
DATASET = 'cora'
# 过滤器
FILTER = 'localpool'  # 'chebyshev'
# 最大多项式的度
MAX_DEGREE = 2  # maximum polynomial degree
# 是否对称正则化
SYM_NORM = True  # symmetric (True) vs. left-only (False) normalization
# 迭代次数
NB_EPOCH = 20000
# 提前停止参数
PATIENCE = 10  # early stopping patience

# 加载数据
# Get data
X, A, y = load_data(dataset=DATASET)        # 特征、邻接矩阵、标签
# 训练集样本标签、验证集样本标签、测试集样本标签、训练集索引列表
# 验证集索引列表、测试集索引列表、训练数据的样本掩码
y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)

# 对特征进行归一化处理
# Normalize X
X /= X.sum(1).reshape(-1, 1)

# 当过滤器为局部池化过滤器时
if FILTER == 'localpool':
    """ Local pooling filters (see 'renormalization trick' in Kipf & Welling, arXiv 2016) """
    print('Using local pooling filters...')

    # 加入自连接的邻接矩阵
    A_ = preprocess_adj(A, SYM_NORM)
    support = 1
    # 特征矩阵和邻接矩阵
    graph = [X, A_]
    G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]
    
# 当过滤器为切比雪夫多项式时
elif FILTER == 'chebyshev':
    """ Chebyshev polynomial basis filters (Defferard et al., NIPS 2016)  """
    print('Using Chebyshev polynomial basis filters...')
    # 对拉普拉斯矩阵进行归一化处理，得到对称规范化的拉普拉斯矩阵
    L = normalized_laplacian(A, SYM_NORM)
    # 重新调整对称归一化的图拉普拉斯矩阵，得到其简化版本
    L_scaled = rescale_laplacian(L)
    # 计算直到MAX_DEGREE阶的切比雪夫多项式
    T_k = chebyshev_polynomial(L_scaled, MAX_DEGREE)
    #
    support = MAX_DEGREE + 1
    # 特征矩阵、直到MAX_DEGREE阶的切比雪夫多项式列表
    graph = [X]+T_k     # 列表相加
    G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(support)]
else:
    raise Exception('Invalid filter type.')

# shape为形状元组，不包括batch_size
# 例如shape=(32, )表示预期的输入将是一批32维的向量
X_in = Input(shape=(X.shape[1],))

# 定义模型架构
# 注意：我们将图卷积网络的参数作为张量列表传递
# 更优雅的做法需要重写Layer基类
# Define model architecture
# NOTE: We pass arguments for graph convolutional layers as a list of tensors.
# This is somewhat hacky, more elegant options would require rewriting the Layer base class.
H = Dropout(0.5)(X_in)
H = GraphConvolution(16, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)
H = Dropout(0.5)(H)
Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)


# 编译模型
# Compile model
model = Model(inputs=[X_in]+G, outputs=Y)
model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))

# 训练过程中的辅助变量
# Helper variables for main training loop
wait = 0
preds = None
best_val_loss = 99999

# 训练模型
# Fit
for epoch in range(1, NB_EPOCH+1):

    # 统计系统时钟的时间戳
    # Log wall-clock time
    t = time.time()

    # 每一次迭代过程
    # Single training iteration (we mask nodes without labels for loss calculation)
    model.fit(graph, y_train, sample_weight=train_mask,     # 向sample_weight参数传递train_mask用于样本掩码
              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\
              
    # 预测模型在整个数据集上的输出
    # Predict on full dataset
    preds = model.predict(graph, batch_size=A.shape[0])
    # 模型在验证集上的损失和准确率
    # Train / validation scores
    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],
                                                   [idx_train, idx_val])
                                                   
    print("Epoch: {:04d}".format(epoch),
          "train_loss= {:.4f}".format(train_val_loss[0]),       # 在训练集上的损失
          "train_acc= {:.4f}".format(train_val_acc[0]),     # 在训练集上的准确率
          "val_loss= {:.4f}".format(train_val_loss[1]),     # 在验证集上的损失
          "val_acc= {:.4f}".format(train_val_acc[1]),       # 在验证集上的准确率
          "time= {:.4f}".format(time.time() - t))       # 本次迭代的运行时间

    # 提取停止
    # Early stopping
    if train_val_loss[1] < best_val_loss:
        best_val_loss = train_val_loss[1]
        wait = 0
    else:
        # 当模型在测试集上的损失连续10次迭代没有优化时，则提取停止
        if wait >= PATIENCE:
            print('Epoch {}: early stopping'.format(epoch))
            break
        wait += 1

# 模型在测试集上的损失和准确率
# Testing
test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])
print("Test set results:",
      "loss= {:.4f}".format(test_loss[0]),
      "accuracy= {:.4f}".format(test_acc[0]))
```

## Reference
作者：纵横
[链接：https://www.zhihu.com/question/54504471/answer/611222866](https://www.zhihu.com/question/54504471/answer/611222866)
来源：知乎  
作者：蝈蝈
[链接：https://www.zhihu.com/question/54504471/answer/730625049](https://www.zhihu.com/question/54504471/answer/730625049)
来源：知乎  
作者：邢翔瑞 
来源：CSDN 
[原文：https://blog.csdn.net/weixin_36474809/article/details/89316439](https://blog.csdn.net/weixin_36474809/article/details/89316439)   





