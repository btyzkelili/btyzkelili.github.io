---
layout:     post
title:      "Deep Learning"
subtitle:   "GCN"
date:       2019-07-24
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Deep Learning
---  
# 图卷积网络(GCN)

## 1. Why GCN?
CNN应用在具有网格结果的数据(Euclidean Domain Data)上，比如图像(图片结构上的平移不变性：一个小窗口无论移动到图片的哪一个位置，其内部的结构都是一模一样的，因此CNN可以实现参数共享。这就是CNN的精髓所在。
)，音频，句子等，但是现实生活中有很多非网格结构的数据，结点连接方式不同(图的结构一般来说是十分不规则的，可以认为是无限维的一种数据，所以它没有平移不变性。每一个节点的周围结构可能都是独一无二的)，例如社交网络、蛋白质结构、知识图谱等。
对于这样的数据，产生了很多解决办法，例如GNN、DeepWalk、node2vec、GCN等。

## 2. GCN架构
设图有N个结点，每个节点有各自的特征，组成N * D维的特征矩阵X，各结点之间形成N * N维的邻接矩阵A，
网络层与层之间传播的方式是：
![](/img/dl_gcn/1.png)  
A˜：A + I<sub>N</sub>(单位矩阵)  
D˜：A˜的度矩阵(degree matrix)，![](/img/dl_gcn/2.png)    
H<sup>l</sup>：第l层的输入特征，对于输入层，H=X  
σ：非线性激活函数  
其中，D˜<sup>-1/2</sup>A˜D˜<sup>-1/2</sup>可以预先计算出来，因为D˜由A˜得到，A˜由A和I得到，A是输入。  

![](/img/dl_gcn/3.png)![](/img/dl_gcn/4.png)  
X和A作为网络的输入，整个训练过程中，**A不变**，输入结点的特征经过GCN之后从X变为Z，对所有带标签的节点计算cross entropy损失函数来调整网络参数，使得输出的Z更好的描述结点

## 3. GCN架构由来
假设图中没有自环，即A<sub>ii</sub>=0：  
GCN本质是通过网络(graph)中的关联信息(edge)来得到节点的特征(node represent)  

假如说，我们要根据X<sub>i</sub>的朋友圈来预测X<sub>i</sub>的工资：
![](/img/dl_gcn/6.png)  

1. 首先考虑，所有朋友的工资的平均值就是自己的工资，所以(平均数值需要对加和的系数归一化即可，后面完善)：
![](/img/dl_gcn/5.png)  
如果是无权图，A<sub>ij</sub>是0/1，如果j不在neighbor(i)中，A<sub>ij</sub>=0,所以，上式可写为：
![](/img/dl_gcn/7.png)  
矩阵形式：
![](/img/dl_gcn/8.png)  

2. 平均方法有一个问题：没有考虑到我们与不同朋友的亲密度是不同的，比如，我们都认识马化腾，但是张志东与马化腾的亲密度要比我们和马化腾的亲密度高得多。因此，可以预测张志东的工资比我们更接近马化腾。
严谨来说，上式忽略了边的权重。我们只需要把无权图(A<sub>ij</sub>只能为0或1)变为有权图(A<sub>ij</sub>可以为任何数)。有很多工作都是在研究如何更巧妙的构建有权图(比如用节点间的相似度等),我们仍可以用上式表达加权平均:
![](/img/dl_gcn/8.png)  

3. 加权平均法也一个小问题：它没有考虑到『你可能很会吹牛』。比如，虽然我们能力和朋友们差不多，但是我们很会吹牛逼所以特别受到领导赏识（误）被提拔的很快。这时，可以预测我们的工资会比我们的朋友要高。
严谨来说，上式忽略了节点自身的特征。因此一般我们会通过添加一个自环把节点自身特征加回来：
![](/img/dl_gcn/9.png)  
通过A + I操作，可以将结点自身的特征加回来

4. 从另一个角度看，有一些任务对工资的绝对值不感兴趣。比如，『检测凤凰男防止相亲采坑』的任务。凤凰男可能会因为某些机遇工资飙升，但是他的朋友圈工资水平仍然不高，从而产生较大的贫富差距。而对于富二代而言，他们的朋友圈通常也是富二代，贫富差距较小。
严谨来说，有些任务可能更关注相邻节点间的“差分”。因此我们一般会用拉普拉斯矩阵L=D-A来做，其中D是图的度矩阵，节点的“差分”可以表示为：
![](/img/dl_gcn/10.png)  
从化简后的公式可以看到，通过拉普拉斯矩阵的相乘，我们求得是以边为权、节点特征的差分。

5. 归一化问题，无论是A+I还是D-A，其实存在一个重大的问题，我们求的是和而不是平均。我们可能会面临『大佬可能没有多少朋友』的问题。在聚合后，即使大佬的朋友圈都是高收入群体，也没办法超过一个拥有低端朋友圈的交际花。
![](/img/dl_gcn/11.png)  
严谨来说，这会导致离群较远或者度较小的节点在聚合后特征较小，离群较近或者度较大的节点在聚合后特征较大。因此，我们需要通过归一化消除这个问题。这里，我们令A˜=A+I或者D-A，其中D˜为A˜的度矩阵：
![](/img/dl_gcn/12.png)  
从化简后的公式可以看到，通过D˜<sup>-1</sup>操作，已经将求和变成了加权平均啦，权值之和归一化为1

6. 对称归一化，解决了平均的问题，我们能不能更进一步呢？现在朋友很少的大佬满意了。然而和朋友很少的大佬交朋友的都是什么人呢？除了高收入的程序员，还会有一些绿茶婊、交际花。表面上看，这些『交际花似乎也是大佬为数不多的朋友之一』，然而她的工资和大佬们天差地别。之所以她与大佬是朋友，是因为她跟每个人几乎都是朋友，想要扩展自己的人脉圈。因此，我们要剔除这一部分交际花对大佬工资预测的影响。
![](/img/dl_gcn/13.png)  
严谨来说，我们除了应该考虑聚合节点i的度D˜<sub>ii</sub>，还应该考虑被聚合节点j的度D˜<sub>jj</sub>。如何同时考虑两者呢，我们可以使用几何平均数：$\sqrt D˜<sub>ii</sub>D˜<sub>jj</sub>$
![](/img/dl_gcn/14.png)  
从化简后的公式可以看到，通过D˜<sup>-0.5</sup>A˜D˜<sup>-0.5</sup>操作，已经在加权时通将D˜<sup>-01</sup>拆分成了D˜<sub>ii</sub>和D˜<sub>jj</sub>的集合平均，从而剔除了被聚合节点j的度的影响。

综上，到这里 GCN 的公式已经进化成完全体了。如何想到D˜<sup>-0.5</sup>A˜D˜<sup>-0.5</sup>这种方法的呢？不难发现，这个公式就是拉普拉斯变换或者说谱聚类加了参数：
1. 首先计算出标准化后的拉普拉斯矩阵D˜<sup>-0.5</sup>(D-A)D˜<sup>-0.5</sup>
2. 将拉普拉斯矩阵进行SVD特征分解，得到U
3. 选取较小的k个U到其他聚类算法，诸如KMeans

## 4. 拉普拉斯矩阵
## 5. GCN.py

---
参考：

作者：纵横
链接：https://www.zhihu.com/question/54504471/answer/611222866
来源：知乎

作者：蝈蝈
链接：https://www.zhihu.com/question/54504471/answer/730625049
来源：知乎





