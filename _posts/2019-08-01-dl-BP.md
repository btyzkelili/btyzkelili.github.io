---
layout:     post
title:      "Deep Learning"
subtitle:   "Backpropagation"
date:       2019-08-01
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Deep Learning
---  
## 1. Backpropagation
![](/img/lhy_ml/bp-1.png)  
计算L的微分，就是计算l和的微分，也就是l的微分和，所以只要知道l的微分，就可以知道L的微分。  
![](/img/lhy_ml/bp-4.png)  
l的微分可以看做是两部分：Forward pass + Backward pass

#### 1.1 Forward pass
![](/img/lhy_ml/bp-2.png)  
![](/img/lhy_ml/bp-3.png)  

#### 1.2 Backward pass
![](/img/lhy_ml/bp-5.png)  
![](/img/lhy_ml/bp-6.png)  
![](/img/lhy_ml/bp-7.png)  
![](/img/lhy_ml/bp-8.png)  
![](/img/lhy_ml/bp-9.png)  
输出层的l对z的微分可以直接计算出来，从输出层往前计算，每层都相当于对所有(反向)输入的 l对z的微分乘以参数后的和 进行了放大，放大倍数是激活函数的导数

#### 1.3 Summary
![](/img/lhy_ml/bp-10.png)  
计算出L的微分之后，就可以用梯度下降的方法更新参数，找最好的f  
不同激励函数、损失函数、网络结构(神经元数、层数等)时的L的微分不同，也就是要找最低点的地形不同，对于一个任务，首先确定损失函数、激励函数，然后调参调整网络结构/model，不同网络结构地形不同，能到达的最低点也不同，要找一个地形最低点最低的网络结构/model，并不是地形不变，去找这个地形的最低点，而是不同地形最低点不同，要找最低点最低的地形，通过调整学习率、batch、epoch等来保证可以到达每个地形的最低点，不用每改变网络结构就重新找学习率等，学习率没有那么敏感。
