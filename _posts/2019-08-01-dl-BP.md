---
layout:     post
title:      "Deep Learning"
subtitle:   "Backpropagation"
date:       2019-08-01
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Deep Learning
---  
## 1. Backpropagation
![](/img/lhy_ml/bp-1.png)  
计算L的微分，就是计算l和的微分，也就是l的微分和，所以只要知道l的微分，就可以知道L的微分。  
![](/img/lhy_ml/bp-4.png)  
l的微分可以看做是两部分：Forward pass + Backward pass

#### 1.1 Forward pass
![](/img/lhy_ml/bp-2.png)  
![](/img/lhy_ml/bp-3.png)  

#### 1.2 Backward pass
![](/img/lhy_ml/bp-5.png)  
![](/img/lhy_ml/bp-6.png)  
![](/img/lhy_ml/bp-7.png)  
![](/img/lhy_ml/bp-8.png)  
![](/img/lhy_ml/bp-9.png)  
输出层的l对z的微分可以直接计算出来，从输出层往前计算，每层都相当于对所有(反向)输出的 l对z的微分乘以参数后的和 进行了放大，放大倍数是激活函数的导数

#### 1.3 Summary
![](/img/lhy_ml/bp-10.png)  
计算出L的微分之后，就可以用梯度下降的方法更新参数，找最好的f
