---
layout:     post
title:      "Machine Learning"
subtitle:   "lhy"
date:       2019-07-25
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Machine Learning
---  
## 0. Machine Learning
![](/img/lhy_ml/1.png) 
在有机器学习之前，实现人工智能的方式：hand-crafied rules，成千上万的if，永远无法超越创造者，
并且需要耗费大量的人力。

那什么是Machine Learning?
**Machine Learning≈Looking for a function**
![](/img/lhy_ml/2.png) 
1. 找一个f的集合，也就是模型
2. 让机器能够根据数据衡量一个f的好坏，通过loss function：输入f输出它的好坏，L(f)=L(w,b)
3. 挑选出最好的f
![](/img/lhy_ml/3.png) 
蓝色部分是scenaio(方案)，是由问题决定的，能用有监督就不应该用强化学习，红色部分是在方案下所选择的task，
绿色部分是每种task下可用的解决方法

## 1. Regression
#### 1.1 Regression Example
提出一个回归问题：  
![](/img/lhy_ml/4.png)   
* 第一步，找一个Model  
![](/img/lhy_ml/5.png)   

* 第二步，衡量f的好坏  
![](/img/lhy_ml/6.png)  
用loss function L来衡量f，L的输入是f，输出是f的好坏，由于f是由w,b构成的，也可以写成L(w,b)，所以f越好就是希望L越小，
使得L最小的f/ w,b记为f*/ w*,b*  

* 第三步，选择最好的f  
对于线性回归可以使用线性代数知识直接计算L的最小值，但无法直接计算其他问题    
在有了L之后只要对输入可以进行微分，就可以使用gradient descent来求解L的最小值  
![](/img/lhy_ml/7.png)   
gradient descent：随机w0,b0，计算L关于w,b的微分(切线斜率)，如果斜率为负，需要增大w，反之需要减小，所以我们将w,b向着梯度的反方向以学习率η倍数增加  

#### 1.2 gradient descent的问题：  
![](/img/lhy_ml/8.png)   
1. 无法保证找到的是global minima  
2. 有可能梯度=0时是saddle point  
3. 有时梯度非常小时，它可能只是一个离minima比较远的平滑的部分  
* 不过，线性回归问题的梯度都是碗型，不存在这些问题

#### 1.3 Regularization
* 更多model选择：  
![](/img/lhy_ml/9.png) 
更复杂的model在训练集上可以获得更小的error
![](/img/lhy_ml/10.png) 
但是在测试集上可能表现不好，出现overfitting现象  

* 解决办法：
1. 重新设计model  
![](/img/lhy_ml/11.png) 
![](/img/lhy_ml/12.png) 
这个model仍然是linear model，linear model是指参数对于output是否是线性，同理b+w1x1+w2x1<sup>2</sup>也是线性模型  

2. 正则化
![](/img/lhy_ml/13.png)   
正则化修改了L，在原来的基础上增加了一项，要求Lmin就是希望原来loss小的同时要选w很小的f，也就是平滑的f，平滑意味着输入有变化时，输出变化小，之所以要这样要求是因为多数情况下平滑的f更可能是较好的f  

为什么不对b正则化？因为b本身是一条直线，对f的平滑程度无影响，对它正则化就是要求b接近0  

λ越大，f越平滑， 一般对λ取值0.01~0.05之间




