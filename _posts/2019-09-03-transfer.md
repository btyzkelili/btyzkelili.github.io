---
layout:     post
title:      "Transfer Learning"
subtitle:   "Transfer Learning"
date:       2019-09-03
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Transfer Learning
---  
## Transfer Learning
![](/img/lhy_ml/trans-1.jpg)  
数据不是直接相关与任务
![](/img/lhy_ml/trans-2.jpg)  

#### Model Fine-tuning(微调)
![](/img/lhy_ml/trans-3.jpg)  
可用于语音辨识任务，source data是很多人的语音，target data是某个人的语音    
通过用source data训练模型，然后用tartget data进行微调，由于target data数量很少容易over fitting，需要对data进行特殊处理：
conservative training

![](/img/lhy_ml/trans-4.jpg)  
![](/img/lhy_ml/trans-5.jpg)  
conservative training就是保守训练，要求新的网络和原来的网络不要相差太多，实做上，可以让相同输入的输出不要相差太多，也可以让网络参数不要相差太大，或者在训练时限制在fine tunning时只调整部分层，如果target data足够的话也可以调整整个网络，
![](/img/lhy_ml/trans-6.jpg)  
在语音任务上通常调整前面几层，而影像任务上通常调整后面几层
![](/img/lhy_ml/trans-7.jpg)  

#### Multitask Learning
![](/img/lhy_ml/trans-8.jpg)  
task A\B有一定的联系

**多任务例子**
![](/img/lhy_ml/trans-9.jpg)  
![](/img/lhy_ml/trans-10.jpg)  
从上图结果上来看，和欧洲语言一起学习时，学习中文效果更好

**Progressive Neural Networks**
![](/img/lhy_ml/trans-11.jpg)  
先学task1，在这个基础上再学task2，task1对task2有帮助，task2用新的NN，但是用到task1的层输出最为自己的层输入，同时固定task1的NN的参数,
但是这样参数会随着task增加而增加，所以有新方法提出

![](/img/lhy_ml/trans-12.jpg)  
一开始就先固定一个大的NN，task1只能用网络中一部分参数，训练好之后固定这部分参数，让下一个task进行训练，通过这种方法来防止参数增加




