---
layout:     post
title:      "Transfer Learning"
subtitle:   "Transfer Learning"
date:       2019-09-03
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Transfer Learning
---  
## Transfer Learning
![](/img/lhy_ml/trans-1.jpg)  
数据不是直接相关与任务
![](/img/lhy_ml/trans-2.jpg)  

#### Model Fine-tuning(微调)
![](/img/lhy_ml/trans-3.jpg)  
可用于语音辨识任务，source data是很多人的语音，target data是某个人的语音    
通过用source data训练模型，然后用tartget data进行微调，由于target data数量很少容易over fitting，需要对data进行特殊处理：
conservative training

![](/img/lhy_ml/trans-4.jpg)  
![](/img/lhy_ml/trans-5.jpg)  
conservative training就是保守训练，要求新的网络和原来的网络不要相差太多，实做上，可以让相同输入的输出不要相差太多，也可以让网络参数不要相差太大，或者在训练时限制在fine tunning时只调整部分层，如果target data足够的话也可以调整整个网络，
![](/img/lhy_ml/trans-6.jpg)  
在语音任务上通常调整前面几层，而影像任务上通常调整后面几层
![](/img/lhy_ml/trans-7.jpg)  

#### Multitask Learning
![](/img/lhy_ml/trans-8.jpg)  
task A\B有一定的联系

**多任务例子**
![](/img/lhy_ml/trans-9.jpg)  
![](/img/lhy_ml/trans-10.jpg)  
从上图结果上来看，和欧洲语言一起学习时，学习中文效果更好

**Progressive Neural Networks**
![](/img/lhy_ml/trans-11.jpg)  
先学task1，在这个基础上再学task2，task1对task2有帮助，task2用新的NN，但是用到task1的层输出最为自己的层输入，同时固定task1的NN的参数,
但是这样参数会随着task增加而增加，所以有新方法提出

![](/img/lhy_ml/trans-12.jpg)  
一开始就先固定一个大的NN，task1只能用网络中一部分参数，训练好之后固定这部分参数，让下一个task进行训练，通过这种方法来防止参数增加

#### Domain-adversarial
![](/img/lhy_ml/trans-17.jpg)  

![](/img/lhy_ml/trans-13.jpg)  ![](/img/lhy_ml/trans-14.jpg)  
看成GAN其中一种，在domain上的应用，将source和target data放到一个domian  
source data和target data的分布不同，不能直接放到网络中使用，而应该放到同一domain中后再使用

![](/img/lhy_ml/trans-15.jpg)  
绿色网络将不同分布的data转换到同一分布/domain中，红色部分的网络要侦测处输入的image是属于source domian还是target
domian，但是只有这样是不够的，因为绿色部分很容易骗过红色，因为他只要让输出全为0就能骗过，所以再增加蓝色部分，绿色
输出的feature要有重要的信息，让蓝色的网络可以正确分类输入的image，绿色的部分要让蓝色网络正确让红色网络错误

![](/img/lhy_ml/trans-16.jpg)  
训练过程中，当蓝色和红色都要gradient descent向各自结果正确的方向，它们要求绿色更新参数时满足它们需求的时候，绿色会将红色的梯度乘以一个负号，做红色NN相反的方向，让红色的无法正确分类，但失败是简单的，在失败过程中，红色部分要奋力挣扎，挣扎过程是重要的，但是实践中难让它去不断挣扎

#### Zero-shot Learning
![](/img/lhy_ml/trans-18.jpg)  
training data 和 testing data是不同任务情况，在语音辨识任务中，training data 和 testing data的词汇没有重复的，需要找出比词汇会更小的单位音标，再把音标组为词汇就可以解决不同任务的问题  

![](/img/lhy_ml/trans-19.jpg)  
![](/img/lhy_ml/trans-20.jpg)  
在影像辨识问题中，每一类图片用其属性表示，网络输出输入图片的属性情况，根据输出的属性判定类别

![](/img/lhy_ml/trans-21.jpg)  
f可以是CNN，将图片输出到CNN中，得到f(x)，再将属性y输入到某个网络/g中，得到g(y)，让同一类的图片f(x)和其属性尽可能相近，当分辨一张新图片时，计算f(x)，找与其最相近的g(x)来判断类别

![](/img/lhy_ml/trans-23.jpg)  
仅要求同一类的图片f(x)和其属性尽可能相近是不够的，还要不同类的尽可能远，但是尽可能远的话网络可能只会让不同类尽可能远而不关注相同类更近，所以要有让同类和不同类相差有范围，要同一类的f(x)和g(y)内积-不同类的f(x)和g(y1)内积>k，防止无限减小

![](/img/lhy_ml/trans-22.jpg)  
如果没有属性数据，可以用类别名的word embedding作为g(y)

![](/img/lhy_ml/trans-24.jpg)  
图片输入到NN中，得到50%可能是lion，50%是tiger，将tiger的word embedding和lion的word embedding都乘以50%后相加，
得到的word enbedding最接近的就是图片的类别

#### Self-taught learning
![](/img/lhy_ml/trans-25.jpg)  
