---
layout:     post
title:      "Deep Learning"
subtitle:   "Tips for Deep Learning"
date:       2019-08-02
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Deep Learning
---  
## 1. 
![](/img/lhy_ml/tip-1.png)  
先确定再训练集上表现好，再看测试集，如果此时测试集表现不好，是overfitting

![](/img/lhy_ml/tip-2.png)  
只根据右边的图看到更复杂的网络在测试集上表现不好，说明是overfitting是错误的，要看训练集的情况，在训练集上更复杂网络表现也不好说明是网络本身
没有训练好的问题，而不是overfitting  
是存在复杂的网络训练集表现更差的情况的，因为他有可能停在了local minima或者saddle point位置，梯度下降并不能保证一定可以找到最好的解

![](/img/lhy_ml/tip-3.png)  
提高训练集的表现：
1. 使用新的激励函数
使用sigmoid函数作为激励函数会产生vanishing Gradient Problem，离输出近的层梯度大，输入近的层梯度小，导致后面层变化快先收敛而前面层的参数
仍然是最开始随即出来的，由于后面层依赖前面层的输出来更新参数，导致后面是根据随机出来的前面参数来更新，没有学到正确东西
![](/img/lhy_ml/tip-4.png)  
![](/img/lhy_ml/tip-5.png)  
Relu激励函数，在输出z<0时，输出a为0，相当于没有该神经元，使得网络变为一个更瘦的线性模型，也就不会让前面层有更小的梯度。该网络在局部是线性的，
但整体上是非线性的
![](/img/lhy_ml/tip-6.png)   
![](/img/lhy_ml/tip-7.png)   ![](/img/lhy_ml/tip-8.png)   ![](/img/lhy_ml/tip-9.png)   
 Maxout是一种网络自己学习激励函数的一种方法，事先确定(参数)一组有多少个元素，一组多少个元素就代表最后的激励函数是由多少个直线取上界构成的，
 ReLU是Maxout的一个特例，训练时，对于max不能微分问题，其实只要能算出参数变化一定量对loss的影响量就可以计算出微分。
 
2. 调整学习率
![](/img/lhy_ml/tip-10.png)   ![](/img/lhy_ml/tip-13.png)   ![](/img/lhy_ml/tip-11.png)   ![](/img/lhy_ml/tip-12.png)   
Momentum(惯性)：在更新方向为梯度的反方向加上惯性的方向来跳出局部最优点
Adam = RMSProp + Momentum

提高测试集的表现
1. 减少轮次
![](/img/lhy_ml/tip-14.png)   

2. 正则化
![](/img/lhy_ml/tip-15.png)   ![](/img/lhy_ml/tip-16.png)   
正则化方法的参数更新过程其实是模拟人脑“遗忘不需要的记忆”，也就是把不需要的参数w=0，给L加上限制，最后参数w会乘以一个小于1的数，也就会一直减少到零，
如果这个参数是有用的(不需要遗忘)，那他的梯度(第二项)就会增加参数的值，直到两边平衡参数训练完成，如果这个参数需要被遗忘，梯度不会增加该参数，这个参数
就会减小到0(Weight Decay)。L1和L2具体使用那个需要尝试

3. dropout
![](/img/lhy_ml/tip-17.png)   ![](/img/lhy_ml/tip-18.png)   
训练时，dropout参数p，相当于改变网络结构，让网络变瘦，每个batch会重新选dropout的神经元，而在**测试时不设置dropout**，并且要把参数乘以(1-p)，直观上理解，dropout在训练时相当于负重训练，在测试时需要将重物取下

![](/img/lhy_ml/tip-19.png)   
Ensemble是用不同的训练数据训练不同结构的网络，将最后得到的结果取平均
![](/img/lhy_ml/tip-20.png)   
dropout是Ensemble的一种






