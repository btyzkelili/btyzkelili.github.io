---
layout:     post
title:      "SVM"
subtitle:   "SVM"
date:       2019-09-04
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - SVM
---  
## SVM
![](/img/lhy_ml/svm-1.jpg)  
SVM = Hinge loss + Kernal Method

#### Hinge loss
![](/img/lhy_ml/svm-2.jpg)  
![](/img/lhy_ml/svm-3.jpg)  
分类问题中，不同loss function图像如上，理想情况图像为黑色，但无法微分，所以要用其他函数代替理想函数，整体上yf(x)
要求同号，且yf(x)越大loss越小，其中square loss=∑(yf(x)-1)<sup>2</sup>红色线，在yf(x)大时loss增大不符合要求，
sigmoid+square loss=∑(σ(yf(x))-1)<sup>2</sup>蓝色线，sigmoid+corss emtropy=ln(1+exp(-yf(x)))(推导)绿色线是理想情况的一个上界，
对比蓝色，在loss值大时，做出同样改变时绿色的loss更显著变小，回报更大，从而更努力向正确的方向(yf(x)增大的方向)去努力

![](/img/lhy_ml/svm-4.jpg)  
Hinge loss=max(0,1-yf(x))，它与sigmoid cross emtropy主要差别在右半部分，在yf(x)>1之后便不再努力，而cross entropy会尽
可能做得更好，之所以是用1，是为了保证hinge loss是理想情况的上界

![](/img/lhy_ml/svm-5.jpg)  
SVM用Hinge loss，虽然通常SVM不用gradient descent训练，但是是能用的，SVM有deep版本

![](/img/lhy_ml/svm-6.jpg)  
通常我们看到的SVM形式是下面红框中的形式，表示yf(x)是同号的并且>=margin 1，但这个margin是soft形式，有时无法满足margin时可以放宽margin，所以ε不能为负值,
这个问题是一个QP问题可以用一个QP问题解法来求解，也可以像上面一样用gradient descent求解

这个通常形式的SVM可以由上面的Hinde loss得到，在求minimize的情况下图中的两个红框互相相等

#### Kernal Method
![](/img/lhy_ml/svm-7.jpg)  
让loss最小的一组w记为w*，w*是数据点的线性组合，因为初始w=0，每次用gradient descent进行更新时，w<-w-η∑c(w)x，其中c(w)
表示loss function的微分，最后更新得到的w是所有x的线性变化组合，当loss funtion是Hinge loss时，会有部分c(w)=0(看上面的Hinge loss图像)，
所以不是所有数据都对最终的w/model有影响，也就是说α*是sparse，把x对应α不是0的x叫做支撑向量，这就是支撑向量机的由来

![](/img/lhy_ml/svm-8.jpg)  
![](/img/lhy_ml/svm-9.jpg)  
把w写成data point的线性组合方便使用kernal method，推导过程如上，所以我们不需要知道vector x(之前需要计算x作为输入之后的输出vector x，再用vector x计算loss)，
只需要知道K(x,z)，然后找到使得L最小的α即可

不止SVM可以用kernal trick，linear/logistic 都可以用

#### Kernel Trick
![](/img/lhy_ml/svm-10.jpg)  
K(x,z)=(x·z)<sup>2</sup>=Φ(x)·Φ(z)，也就是说(x·z)<sup>2</sup>=先将x,z特征转化再内积，这里特征转换是指存在x1,x2...之间的所有组合情况，
不考虑它们的系数，系数可以之后再乘上去

![](/img/lhy_ml/svm-11.jpg)  
Radial Basis Function Kernel K(x,z)=exp(-1/2//x-z//<sub>2</sub>)表示在无穷多维平面上操作，不过无穷多维容易overfitting

![](/img/lhy_ml/svm-12.jpg)  
Sigmoid Kernel K(x,z)=tanh(x,z)相当于有一层隐层的网络

![](/img/lhy_ml/svm-13.jpg)  
Mercer's theory可以检测自定的Kernel funtion背后是否是两个vector内积的结果，如何定Kernel见上面的参考

![](/img/lhy_ml/svm-14.jpg)  

#### deep learning ans SVM
![](/img/lhy_ml/svm-14.jpg)  
当只有一个Kernel function时就好像只有一层的网络，对Kernel function进行线性转换就相当于有多层网络





