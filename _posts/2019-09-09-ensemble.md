---
layout:     post
title:      "Ensemble"
subtitle:   "Ensemble"
date:       2019-09-09
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Ensemble
---  
## Ensemble
Ensemble是合作打怪，而且各司其职
#### Bagging
![](/img/lhy_ml/ensemble-20.jpg)  
Sampling方法做bagging  
![](/img/lhy_ml/ensemble-1.jpg)  
model已经很复杂容易overfit/bias小variance大(例如decision tree)时用bagging  
回归问题用平均，分类问题用投票  
**Decision Tree**
![](/img/lhy_ml/ensemble-2.jpg)  
只要想，decision tree可以做到正确率100%，很容易overfitting，对decision tree用bagging，也就是random forest
![](/img/lhy_ml/ensemble-3.jpg)  
直接用前面讲的resampling方法得到每一棵树之间没有差太多，最后bagging起来效果不太够，所以在每一次产生树的枝干的时候，都随机决定哪些feature/
question不能用，这样即使用了相同的数据产生的树也会不一样，把树的结果集合起来得到的就是random forest    

**Out of bag validation for bagging**   
bagging方法下的validation，每一个model/f只用了部分data，可以用f2+f4来测试没用用到的数据x1，把最后得到的所有error做平均

#### Boosting(AdaBoost)
boosting方法适用于弱的model  
![](/img/lhy_ml/ensemble-4.jpg)  
boosting可以把低于50%准确率的分类器做到100%  
f之间是有顺序的，有f1才能知道怎么制造出f2，用f2来帮助f1，后面f3...同理  

**Adaboost**  
![](/img/lhy_ml/ensemble-6.jpg)  
初始数据每个data的权重u1=1，用初始data训练f1，然后将u1换成u2，也就是重新对每个data加上不同的权重之后，让error rate ε=0.5，用新的u2 weight data来训练f2，如何修改data的权重呢？  
Z：normalization  
没有办法让error rate>0.5，因为如果分类器的error rate>0.5，只要把它的输出反过来就会小于0.5
![](/img/lhy_ml/ensemble-5.jpg)  
![](/img/lhy_ml/ensemble-7.jpg)  
重新weight data的方法：增大答错的data的weight，也就是给答错的乘以d，减小答对的weight，也就是给答对的除以d，那么d的值是多少呢？
![](/img/lhy_ml/ensemble-8.jpg)  
![](/img/lhy_ml/ensemble-9.jpg)  
求解d<sub>t</sub>的值让新的error rate=0.5，得到d=右下角的结果
![](/img/lhy_ml/ensemble-10.jpg)  
红色框内可写为一个式子：u<sub>t+1</sub><-u<sub>t</sub>* exp(-yf(x)α)，通过上面过程我们得到了一系列f，那我们如何把它们整合使用呢？  
![](/img/lhy_ml/ensemble-11.jpg)  
让最终的H(x)=sign(∑αf(x))，其中α<sub>t</sub>意义：错误率小的f会有比较大的权重

**Adaboost的一个特点**  
![](/img/lhy_ml/ensemble-12.jpg)  
![](/img/lhy_ml/ensemble-13.jpg)  
error rate = 0是有问题的  
Adaboost有一个特点：增加f的数量在train上error没有再下降，但在test上还在下降，这是因为增大f的数量之后会有更大的margin=yg(x)=y∑αf(x)，
AadaBoost如上图红色线条，在y于g(x)同号之后，增大f数量可以继续往右让yg(x)/margin更大使分类器做的更好，
也就会出现在train时error rate=0不再变化但是在test上表现会继续变好的特点

#### Gradient Boosting
Gradient Boosting是boosting的一个general版本
![](/img/lhy_ml/ensemble-14.jpg)  
![](/img/lhy_ml/ensemble-15.jpg)  
![](/img/lhy_ml/ensemble-16.jpg)  
![](/img/lhy_ml/ensemble-17.jpg)  
可以把Adaboost看作是在做gradient descent，只是gradient是一个function，并且有很好的方法可以决定学习率，
但是gradient boosting可以定不同的目标函数

#### Stacking
![](/img/lhy_ml/ensemble-19.jpg)  
我们可以对多个系统的输出直接进行majority vote，但是这种方法没有考虑某些系统可能十分差，不应该和其他系统有相同的权重
![](/img/lhy_ml/ensemble-18.jpg)  
用stacking的方法，把多个系统输出看做新的feature，用final classifier学习权重，把training data分成两部分，final classifier一定要用新的data，因为有可能前面某个system用很奇怪的方法把data硬记下来，后面final classifier如果用相同的data会误以为这个system的表现很好
