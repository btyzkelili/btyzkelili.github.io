---
layout:     post
title:      "Unsupervised Learning"
subtitle:   "Word Embedding"
date:       2019-08-08
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Unsupervised Learning
---  
## 1.Word Embedding
![](/img/lhy_ml/em-1.png)  
![](/img/lhy_ml/em-2.png)  
两类探索文章的方法，一个是对两个词做内积，让得到的结果与这两个词在文章中出现的次数相同，这个次数衡量了这两个词的相关性。另一种方法是predition based：

#### Prediction-based
![](/img/lhy_ml/em-3.png)  
输入前一个词/word(word是一个词不是字，中文如何划分词也是一个问题)，让输出尽可能与后一个次接近minmizing corss entropy，可应用于推文接话，Predicition baed方法还可以用在Language Modeling: 
预测一个句子出现的几率：  
![](/img/lhy_ml/em-4.png)  
由于收集的文章句子有限，给的句子很可能没有，它的几率是0，解决这个问题可以用NN，如上图方式，或者直接输入整个句子，输出这个句子的概率(Bengio,2003)，
常用于语音辨识和机器翻译

![](/img/lhy_ml/em-5.png)  
通常只用一层网络(也就是线性模型，类似PCA)来进行word embedding，之所以不用deep是因为线性的表现也很好，而且一般得到word embedding之后还会接其他NLP工作，后面会有
deep模型，防止训练太过费时

![](/img/lhy_ml/em-6.png)  
也可以同时输入多个word，防止参数过多，可以让这几个输入的word共享参数

![](/img/lhy_ml/em-7.png)  
其他prediction-based方法，CBOW是输入两边的词，让输出与中间的词尽可能相同，Skip-gram是输入中间的词，让输出和两边的词尽可能相同

![](/img/lhy_ml/em-8.png)  
两个语言各自word embedding，再用一些已知的相同含义的词对，训练一个transform，可以找到更多含义相同的词对

![](/img/lhy_ml/em-9.png)  
![](/img/lhy_ml/em-10.png)  
用bag of word会有问题：  
![](/img/lhy_ml/em-11.png)  
![](/img/lhy_ml/em-12.png)  


