---
layout:     post
title:      "Unsupervised Learning"
subtitle:   "Word Embedding"
date:       2019-08-26
author:     "btyzkelili"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Unsupervised Learning
---  
## 1.LLE
![](/img/lhy_ml/n-1.png)  
![](/img/lhy_ml/n-2.png)  
在原来空间中对每个x<sub>i</sub>选择k个邻居，让他们之间的距离尽可能小，的=得到W<sub>ij</sub>，在新的(低维)空间中，用同样的w<sub>ij</sub>让对应的
两个点距离也尽可能小，来得到低维点的表示  
![](/img/lhy_ml/n-3.png)  
对于邻居k数k要选择合适的值  

##2.Laplacian Eigenmaps
![](/img/lhy_ml/n-4.png)  
在对半监督介绍一节中，提到式子S衡量无标签数据的smooth程度(Smoothness假设)
![](/img/lhy_ml/n-5.png)  
将S用在这里，如果两个点在原来空间中距离够近，那么他们在新的(低维)空间中也够近，也就是更smooth，但是这个式子有一个不合适的解，就是让所有的点都在一个位置，
为了防止这种情况，还要限制这些点张满R<sup>M</sup>空间  

3.t-SNE
![](/img/lhy_ml/n-6.png)  
之前的方法都限制了距离够近的两个点在新空间下也要距离够近，但是没有说明距离远的两个带点要如何，会使得给类型的点聚在一起：
![](/img/lhy_ml/n-7.png)  ![](/img/lhy_ml/n-8.png)  ![](/img/lhy_ml/n-9.png)  
对于原空间和新空间，计算任意两点之间的分布P、Q，损失函数L让P、Q之间越相似越好(KL衡量两个分布之间的相似性)
对于计算距离的函数S，t-SNE选择的是1/(1+//  //)，对比exp(-//  //)，在两个点距离较小时，差距不大，但距离较大时会加大/放大这两个点之间的距离

![](/img/lhy_ml/n-6.png)  


